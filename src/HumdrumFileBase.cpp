//
// Programmer:    Craig Stuart Sapp <craig@ccrma.stanford.edu>
// Creation Date: Sat Aug  8 12:24:49 PDT 2015
// Last Modified: Fri Aug 14 21:57:09 PDT 2015
// Filename:      HumdrumFileBase.cpp
// URL:           https://github.com/craigsapp/humlib/blob/master/src/HumdrumFileBase.cpp
// Syntax:        C++11
// vim:           syntax=cpp ts=3 noexpandtab nowrap
//
// Description:   Used to store Humdrum text lines from input stream
//                for further parsing.  This class analyzes the basic
//                spine structure after reading a Humdrum file.  The
//                HumdrumFileStructure class continues structural analysis,
//                primarily of rhythm (generated by **kern, **recip and
//                **koto data) and global/local parameters.
//

#include "HumdrumFileBase.h"
#include "Convert.h"
#include "HumRegex.h"

#include <sstream>
#include <fstream>
#include <stdarg.h>
#include <string.h>

namespace hum {

// START_MERGE


//////////////////////////////
//
// HumdrumFileBase::HumdrumFileBase -- HumdrumFileBase constructor.
//

HumdrumFileBase::HumdrumFileBase(void) : HumHash() {
	addToTrackStarts(NULL);
	m_ticksperquarternote = -1;
	m_quietParse = false;
	m_segmentlevel = 0;
}

HumdrumFileBase::HumdrumFileBase(const string& filename) : HumHash() {
	addToTrackStarts(NULL);
	m_ticksperquarternote = -1;
	m_quietParse = false;
	m_segmentlevel = 0;
	read(filename);
}

HumdrumFileBase::HumdrumFileBase(istream& contents) : HumHash() {
	addToTrackStarts(NULL);
	m_ticksperquarternote = -1;
	m_quietParse = false;
	m_segmentlevel = 0;
	read(contents);
}


//
// HumdrumFileStructure::analyzeStructure() needs to be called after
// using the following constructor:
//

HumdrumFileBase::HumdrumFileBase(HumdrumFileBase& infile) {

	m_filename = infile.m_filename;
	m_segmentlevel = infile.m_segmentlevel;
	m_trackstarts.clear();
	m_trackends.clear();
	m_barlines.clear();
	m_ticksperquarternote = infile.m_ticksperquarternote;
	m_idprefix = infile.m_idprefix;
	m_strand1d.clear();
	m_strand2d.clear();
	m_quietParse = infile.m_quietParse;
	m_parseError = infile.m_parseError;
	m_displayError = infile.m_displayError;

	m_lines.resize(infile.m_lines.size());
	for (int i=0; i<(int)m_lines.size(); i++) {
		m_lines[i] = new HumdrumLine(infile.m_lines[i]->getText());
		m_lines[i]->setOwner(this);
	}

	analyzeBaseFromLines();
}



//////////////////////////////
//
// HumdrumFileBase::operator = -- HumdrumFileStructure::analyzeStructure() 
// needs to be called after copying from another HumdrumFile.
//
//

HumdrumFileBase& HumdrumFileBase::operator=(HumdrumFileBase& infile) {
	if (this == &infile) {
		return *this;
	}

	m_filename = infile.m_filename;
	m_segmentlevel = infile.m_segmentlevel;
	m_trackstarts.clear();
	m_trackends.clear();
	m_barlines.clear();
	m_ticksperquarternote = infile.m_ticksperquarternote;
	m_idprefix = infile.m_idprefix;
	m_strand1d.clear();
	m_strand2d.clear();
	m_quietParse = infile.m_quietParse;
	m_parseError = infile.m_parseError;
	m_displayError = infile.m_displayError;

	m_lines.resize(infile.m_lines.size());
	for (int i=0; i<(int)m_lines.size(); i++) {
		m_lines[i] = new HumdrumLine(infile.m_lines[i]->getText());
		m_lines[i]->setOwner(this);
	}

	analyzeBaseFromLines();
	return *this;
}



//////////////////////////////
//
// HumdrumFileBase::~HumdrumFileBase -- HumdrumFileBase deconstructor.
//

HumdrumFileBase::~HumdrumFileBase() {
	clear();
}



//////////////////////////////
//
// HumdrumFileBase::clear -- Reset the contents of a file to be empty.
//

void HumdrumFileBase::clear(void) {
	// delete memory allocation:
	for (int i=0; i<(int)m_lines.size(); i++) {
		if (m_lines[i] != NULL) {
			delete m_lines[i];
			m_lines[i] = NULL;
		}
	}
	m_lines.clear();

	// clear state variables which are now invalid:
	m_trackstarts.clear();
	m_trackends.clear();
	m_barlines.clear();
	m_ticksperquarternote = -1;
	m_idprefix.clear();
	m_strand1d.clear();
	m_strand2d.clear();
	m_filename.clear();
	m_segmentlevel = 0;
}



//////////////////////////////
//
// HumdrumFileBase::setXmlIdPrefix -- Set the prefix for a HumdrumXML ID
//     atrribute.  The prefix should not start with a digit, nor have
//     spaces in it.
//

void HumdrumFileBase::setXmlIdPrefix(const string& value) {
	m_idprefix = value;
}



//////////////////////////////
//
// HumdrumFileBase::getXmlIdPrefix -- Return the HumdrumXML ID attribute prefix.
//

string HumdrumFileBase::getXmlIdPrefix(void) {
	return m_idprefix;
}



//////////////////////////////
//
// HumdrumFileBase::operator[] -- Access a Humdrum file line by and index.
//    Negative values reference the end of the list of lines.
//

HumdrumLine& HumdrumFileBase::operator[](int index) {
	if (index < 0) {
		index = (int)m_lines.size() - index;
	}
	if ((index < 0) || (index >= (int)m_lines.size())) {
		cerr << "Error: invalid index: " << index << endl;
		index = (int)m_lines.size()-1;
	}
	return *m_lines[index];
}



//////////////////////////////
//
// HumdrumFileBase::setParseError -- Set an error message from parsing
//     input data.  The size of the message will keep track of whether
//     or not an error was generated.  If no error message is generated
//     when reading data, then the parsing of the data is assumed to be
//     good.
//

bool HumdrumFileBase::setParseError(const string& err) {
	m_parseError = err;
	return !m_parseError.size();
}


bool HumdrumFileBase::setParseError(stringstream& err) {
	m_parseError = err.str();
	return !m_parseError.size();
}


bool HumdrumFileBase::setParseError(const char* format, ...) {
	char buffer[1024] = {0};
	va_list ap;
	va_start(ap, format);
	snprintf(buffer, 1024, format, ap);
	va_end(ap);
	m_parseError = buffer;
	return !m_parseError.size();
}



//////////////////////////////
//
// HumdrumFileBase::read -- Load file contents from an input stream or file.
//

bool HumdrumFileBase::read(const string& filename) {
	m_displayError = true;
	return HumdrumFileBase::read(filename.c_str());
}


bool HumdrumFileBase::read(const char* filename) {
	string fname = filename;
	m_displayError = true;

#ifdef USING_URI
	if (fname.find("://") != string::npos) {
		if (Convert::startsWith(fname, "http://")) {
			readFromHttpUri(fname);
			return isValid();
		}
		if (Convert::startsWith(fname, "jrp://")) {
			readFromJrpUri(fname);
			return isValid();
		}
		if (Convert::startsWith(fname, "h://") ||
			Convert::startsWith(fname, "hum://") ||
			Convert::startsWith(fname, "humdrum://")) {
			readFromHumdrumUri(fname);
			return isValid();
		}
	}
#endif

	ifstream infile;
	if (fname.empty() || (fname ==  "-")) {
		return HumdrumFileBase::read(cin);
	} else {
		infile.open(filename);
		if (!infile.is_open()) {
			return setParseError("Cannot open file %s for reading.", filename);
		}
	}
	HumdrumFileBase::read(infile);
	infile.close();
	return isValid();
}


bool HumdrumFileBase::read(istream& contents) {
	clear();
	m_displayError = true;
	char buffer[123123] = {0};
	HumdrumLine* s;
	while (contents.getline(buffer, sizeof(buffer), '\n')) {
		s = new HumdrumLine(buffer);
		s->setOwner(this);
		m_lines.push_back(s);
	}
	return analyzeBaseFromLines();
/*
	if (!analyzeTokens()) { return isValid(); }
	if (!analyzeLines() ) { return isValid(); }
	if (!analyzeSpines()) { return isValid(); }
	if (!analyzeLinks() ) { return isValid(); }
	if (!analyzeTracks()) { return isValid(); }
	return isValid();
*/
}



//////////////////////////////
//
// HumdrumFileBase::readCsv -- Read a Humdrum file in CSV format
//    (rather than TSV format).
// default value: separator = ","
//

bool HumdrumFileBase::readCsv(const string& filename, const string& separator) {
	return HumdrumFileBase::readCsv(filename.c_str());
}


bool HumdrumFileBase::readCsv(const char* filename, const string& separator) {
	ifstream infile;
	if ((strlen(filename) == 0) || (strcmp(filename, "-") == 0)) {
		return HumdrumFileBase::readCsv(cin, separator);
	} else {
		infile.open(filename);
		if (!infile.is_open()) {
			return setParseError("Cannot open file %s for reading.", filename);
		}
	}
	HumdrumFileBase::readCsv(infile, separator);
	infile.close();
	return isValid();
}


bool HumdrumFileBase::readCsv(istream& contents, const string& separator) {
	m_displayError = true;
	char buffer[123123] = {0};
	HumdrumLine* s;
	while (contents.getline(buffer, sizeof(buffer), '\n')) {
		s = new HumdrumLine;
		s->setLineFromCsv(buffer);
		s->setOwner(this);
		m_lines.push_back(s);
	}
	return analyzeBaseFromLines();
}



//////////////////////////////
//
// HumdrumFileBase::analyzeBaseFromLines --
//

bool HumdrumFileBase::analyzeBaseFromLines(void)  {
	if (!analyzeTokens()) { return isValid(); }
	if (!analyzeLines() ) { return isValid(); }
	if (!analyzeSpines()) { return isValid(); }
	if (!analyzeLinks() ) { return isValid(); }
	if (!analyzeTracks()) { return isValid(); }
	return isValid();
}



//////////////////////////////
//
// HumdrumFileBase::readString -- Read contents from a string rather than
//    an istream or filename.
//

bool HumdrumFileBase::readString(const string& contents) {
	stringstream infile;
	infile << contents;
	int status = read(infile);
	return status;
}


bool HumdrumFileBase::readString(const char* contents) {
	stringstream infile;
	infile << contents;
	return read(infile);
}



//////////////////////////////
//
// HumdrumFileBase::readStringCsv -- Reads Humdrum data in CSV format.
//

bool HumdrumFileBase::readStringCsv(const char* contents,
		const string& separator) {
	stringstream infile;
	infile << contents;
	return readCsv(infile, separator);
}


bool HumdrumFileBase::readStringCsv(const string& contents,
		const string& separator) {
	stringstream infile;
	infile << contents;
	return readCsv(infile, separator);
}



//////////////////////////////
//
// HumdrumFileBase::getParseError -- Return parse fail reason.
//

string HumdrumFileBase::getParseError(void) const {
	return m_parseError;
}



//////////////////////////////
//
// HumdrumFileBase::isValid -- Returns true if last read was
//     successful.
//

bool HumdrumFileBase::isValid(void) {
	if (m_displayError && (m_parseError.size() > 0)&& !isQuiet()) {
		cerr << m_parseError << endl;
		m_displayError = false;
	}
	return m_parseError.empty();
}



//////////////////////////////
//
// HumdrumFileBase::setQuietParsing -- Prevent error messages from
//   being displayed when reading data.
// @SEEALSO: setNoisyParsing
// @SEEALSO: isQuiet
//

void HumdrumFileBase::setQuietParsing(void) {
	m_quietParse = true;
}



//////////////////////////////
//
// HumdrumFileBase::setFilename --
//

void HumdrumFileBase::setFilename(const string& filename) {
	m_filename = filename;
}



//////////////////////////////
//
// HumdrumFileBase::getFilename --
//

string HumdrumFileBase::getFilename(void) {
	return m_filename;
}



//////////////////////////////
//
// HumdrumFileBase::printSegmentLabel --
//

ostream& HumdrumFileBase::printSegmentLabel(ostream& out) {
	out << "!!!!SEGMENT";
	string filename = getFilename();
	int segment = getSegmentLevel();
	if (segment != 0) {
		if (segment < 0) {
			out << segment;
		} else {
			out << "+" << segment;
		}
	}
	out << ": " << filename << endl;
	return out;
}



//////////////////////////////
//
// HumdrumFileBase::printNonemptySegmentLabel --
//

ostream& HumdrumFileBase::printNonemptySegmentLabel(ostream& out) {
	if (getFilename().size() > 0) {
		printSegmentLabel(out);
	}
	return out;
}



//////////////////////////////
//
// HumdrumFileBase::getSegmentLevel -- return the segment level
//

int HumdrumFileBase::getSegmentLevel(void) {
	return m_segmentlevel;
}



//////////////////////////////
//
// HumdrumFileBase::setSegmentLevel -- return the segment level
//

void HumdrumFileBase::setSegmentLevel(int level) {
	m_segmentlevel = level;
}

//////////////////////////////
//
// HumdrumFileBase::setNoisyParsing -- Display error messages
//   on console when reading data.
// @SEEALSO: setQuietParsing
// @SEEALSO: isQuiet
//

void HumdrumFileBase::setNoisyParsing(void) {
	m_quietParse = false;
}



//////////////////////////////
//
// HumdrmFileBase::isQuiet -- Returns true if parsing errors
//    messages should be suppressed. By default the parsing
//    is "noisy" and the error messages will be printed to
//    standard error.
// @SEEALSO: setQuietParsing
// @SEEALSO: setNoisyParsing
//

bool HumdrumFileBase::isQuiet(void) const{
	return m_quietParse;
}



//////////////////////////////
//
// HumdrumFileBase::printCsv -- print Humdrum file content in
//     CSV format.
// default value: out = std::cout
// default value: separator = ","
//

ostream& HumdrumFileBase::printCsv(ostream& out,
		const string& separator) {
	for (int i=0; i<getLineCount(); i++) {
		((*this)[i]).printCsv(out, separator);
	}
	return out;
}



//////////////////////////////
//
// HumdrumFileBase::printFieldNumber --
//

ostream& HumdrumFileBase::printFieldNumber(int fieldnum, ostream& out) {
	return printFieldIndex(fieldnum - 1, out);
}



//////////////////////////////
//
// HumdrumFileBase::printFieldIndex --
//

ostream& HumdrumFileBase::printFieldIndex(int fieldind, ostream& out) {
	if (fieldind < 0) {
		return out;
	}
	HumdrumFileBase& infile = *this;
	for (int i=0; i<infile.getLineCount(); i++) {
		if (!infile[i].hasSpines()) {
			out << infile[i] << endl;
			continue;
		}
		cout << infile.token(i,fieldind) << endl;
	}
	return out;
}



//////////////////////////////
//
// HumdrumFileBase::getLine -- Return a pointer to the line at a
//     given index in the data storage.
//

HumdrumLine* HumdrumFileBase::getLine(int index) {
	if (index < 0) {
		return NULL;
	} else if (index >= (int)m_lines.size()) {
		return NULL;
	} else {
		return m_lines[index];
	}
}



//////////////////////////////
//
// HumdrumFileBase::analyzeTokens -- Generate token array from
//    current contents of the lines.  If either tokens or the line
//    is changed, then the other state becomes invalid.
//    See createLinesFromTokens for regeneration of lines from tokens.
//

bool HumdrumFileBase::analyzeTokens(void) {
	int i;
	for (i=0; i<(int)m_lines.size(); i++) {
		m_lines[i]->createTokensFromLine();
	}
	return isValid();
}



//////////////////////////////
//
// HumdrumFileBase::createLinesFromTokens -- Generate Humdrum lines strings
//   from the stored list of tokens.
//

void HumdrumFileBase::createLinesFromTokens(void) {
	for (int i=0; i<(int)m_lines.size(); i++) {
		m_lines[i]->createLineFromTokens();
	}
}



////////////////////////////
//
// HumdrumFileBase::appendLine -- Add a line to the file's contents.  The file's
//    spine and rhythmic structure should be recalculated after an append.
//

void HumdrumFileBase::appendLine(const char* line) {
	HumdrumLine* s = new HumdrumLine(line);
	m_lines.push_back(s);
}


void HumdrumFileBase::appendLine(const string& line) {
	HumdrumLine* s = new HumdrumLine(line);
	m_lines.push_back(s);
}


void HumdrumFileBase::appendLine(HumdrumLine* line) {
	// deletion will be handled by class.
	m_lines.push_back(line);
}



////////////////////////////
//
// HumdrumFileBase::appendLine -- Add a line to the file's contents.  The file's
//    spine and rhythmic structure should be recalculated after an append.
//


void HumdrumFileBase::insertLine(int index, const char* line) {
	HumdrumLine* s = new HumdrumLine(line);
	m_lines.insert(m_lines.begin() + index, s);
}


void HumdrumFileBase::insertLine(int index, const string& line) {
	HumdrumLine* s = new HumdrumLine(line);
	m_lines.insert(m_lines.begin() + index, s);
}


void HumdrumFileBase::insertLine(int index, HumdrumLine* line) {
	// deletion will be handled by class.
	m_lines.insert(m_lines.begin() + index, line);
}



//////////////////////////////
//
// HumdrumFileBase::back --
//

HumdrumLine* HumdrumFileBase::back(void) {
	return m_lines.back();
}



//////////////////////////////
//
// HumdrumFileBase::getReferenceRecords --
//

vector<HumdrumLine*> HumdrumFileBase::getReferenceRecords(void) {
	vector<HumdrumLine*> hlps;
	hlps.reserve(32);
	HumdrumLine* hlp;
	auto& infile = *this;
	for (int i=0; i<infile.getLineCount(); i++) {
		if (infile[i].isReference()) {
			hlp = &infile[i];
			hlps.push_back(hlp);
		}
	}
	return hlps;
}



////////////////////////////
//
// HumdrumFileBase::getLineCount -- Returns the number of lines.
//

int HumdrumFileBase::getLineCount(void) const {
	return (int)m_lines.size();
}



//////////////////////////////
//
// HumdrumFileBase::token -- Return the token at the given line/field index.
//

HTp HumdrumFileBase::token(int lineindex, int fieldindex) {
	if (lineindex < 0) {
		lineindex += getLineCount();
	}
	return m_lines[lineindex]->token(fieldindex);
}


//
// Special case that returns a subtoken string:
//   default value separator = " "
//

string HumdrumFileBase::token(int lineindex, int fieldindex,
		int subtokenindex, const string& separator) {
	return token(lineindex, fieldindex)->getSubtoken(subtokenindex, separator);
}



//////////////////////////////
//
// HumdrumFileBase::getMaxTrack -- Returns the number of primary
//     spines in the data.
//

int HumdrumFileBase::getMaxTrack(void) const {
	return (int)m_trackstarts.size() - 1;
}



//////////////////////////////
//
// HumdrumFileBase::printSpineInfo -- Print the spine information for all
//    lines/tokens in file (for debugging).
//

ostream& HumdrumFileBase::printSpineInfo(ostream& out) {
	for (int i=0; i<getLineCount(); i++) {
		m_lines[i]->printSpineInfo(out) << '\n';
	}
	return out;
}



//////////////////////////////
//
// HumdrumFileBase::printDataTypeInfo -- Print the data type for all
//     spines in the file (for debugging).
//

ostream& HumdrumFileBase::printDataTypeInfo(ostream& out) {
	for (int i=0; i<getLineCount(); i++) {
		m_lines[i]->printDataTypeInfo(out) << '\n';
	}
	return out;
}



//////////////////////////////
//
// HumdrumFileBase::printTrackInfo -- Print the track numbers for all
//     tokens in the file (for debugging).
//

ostream& HumdrumFileBase::printTrackInfo(ostream& out) {
	for (int i=0; i<getLineCount(); i++) {
		m_lines[i]->printTrackInfo(out) << '\n';
	}
	return out;
}



//////////////////////////////
//
// HumdrumFileBase::getSpineStartList -- Return a list of the exclustive
//     interpretations starting spines in the data.  The single parameter
//     version of the fuction returns all starting exclusive interpretations.
//     The two-parameter version will result all exclusive interpretations
//     of a given datatype, and the three-parameter version where the third
//     parameter is a vector of string, will selectively include all starting
//     tokens which match one of the data types in the input list.  The
//     trackstarts class variable contains an empty slot at index 0;
//     this is removed in the return vector.
//

void HumdrumFileBase::getSpineStartList(vector<HTp>& spinestarts) {
	spinestarts.reserve(m_trackstarts.size());
	spinestarts.resize(0);
	for (int i=1; i<(int)m_trackstarts.size(); i++) {
		spinestarts.push_back(m_trackstarts[i]);
	}
}


void HumdrumFileBase::getSpineStartList(vector<HTp>& spinestarts,
		const string& exinterp) {
	spinestarts.reserve(m_trackstarts.size());
	spinestarts.resize(0);
	for (int i=1; i<(int)m_trackstarts.size(); i++) {
		if (exinterp == *m_trackstarts[i]) {
			spinestarts.push_back(m_trackstarts[i]);
		}
	}
}


void HumdrumFileBase::getSpineStartList(vector<HTp>& spinestarts,
		const vector<string>& exinterps) {
	spinestarts.reserve(m_trackstarts.size());
	spinestarts.resize(0);
	for (int i=1; i<(int)m_trackstarts.size(); i++) {
		for (int j=0; j<(int)exinterps.size(); j++) {
			if (exinterps[j] == *m_trackstarts[i]) {
				spinestarts.push_back(m_trackstarts[i]);
			}
		}
	}
}


void HumdrumFileBase::getKernSpineStartList(vector<HTp>& spinestarts) {
	getSpineStartList(spinestarts, "**kern");
}

vector<HTp> HumdrumFileBase::getKernSpineStartList(void) {
	vector<HTp> starts;
	HumdrumFileBase::getKernSpineStartList(starts);
	return starts;
}



//////////////////////////////
//
// getPrimaryspineSequence -- Return a list of the HumdrumTokens in a spine,
//    but not any secondary spine content if the spine splits.
//


void HumdrumFileBase::getPrimarySpineSequence(vector<HTp>& sequence, int spine,
		int options) {
	getPrimaryTrackSequence(sequence, spine+1, options);
}



//////////////////////////////
//
// getPrimaryspineSequence -- Return a list of the HumdrumTokens in a spine,
//    but not any secondary spine content if the spine splits.
//


void HumdrumFileBase::getSpineSequence(vector<vector<HTp> >& sequence,
		HTp starttoken, int options) {
	getTrackSequence(sequence, starttoken, options);

}


void HumdrumFileBase::getSpineSequence(vector<vector<HTp> >& sequence,
		int spine, int options) {
	getTrackSequence(sequence, spine+1, options);
}



//////////////////////////////
//
// HumdrumFileBase::getPrimaryTrackSequence -- Return a list of the
//     given primary spine tokens for a given track (indexed starting at
//     one and going through getMaxTrack().
//

void HumdrumFileBase::getPrimaryTrackSequence(vector<HTp>& sequence, int track,
		int options) {
	vector<vector<HTp> > tempseq;
	getTrackSequence(tempseq, track, options | OPT_PRIMARY);
	sequence.resize(tempseq.size());
	for (int i=0; i<(int)tempseq.size(); i++) {
		sequence[i] = tempseq[i][0];
	}
}



/////////////////////////////
//
// HumdrumFileBase::getTrackSequence -- Extract a sequence of tokens
//    for the given spine.  All subspine tokens will be included.
//    See getPrimaryTrackSequence() if you only want the first subspine for
//    a track on all lines.
//
// The following options are used for the getPrimaryTrackTokens:
// * OPT_PRIMARY    => only extract primary subspine/subtrack.
// * OPT_NOEMPTY    => don't include null tokens in extracted list if all
//                        extracted subspines contains null tokens.
//                        Includes null interpretations and comments as well.
// * OPT_NONULL     => don't include any null tokens in extracted list.
// * OPT_NOINTERP   => don't include interprtation tokens.
// * OPT_NOMANIP    => don't include spine manipulators (*^, *v, *x, *+,
//                        but still keep ** and *0).
// * OPT_NOCOMMENT  => don't include comment tokens.
// * OPT_NOGLOBAL   => don't include global records (global comments, reference
//                        records, and empty lines). In other words, only return
//                        a list of tokens from lines which hasSpines() it true.
// * OPT_NOREST     => don't include **kern rests.
// * OPT_NOTIE      => don't include **kern secondary tied notes.
// Compound options:
// * OPT_DATA      (OPT_NOMANIP | OPT_NOCOMMENT | OPT_NOGLOBAL)
//     Only data tokens (including barlines)
// * OPT_ATTACKS   (OPT_DATA | OPT_NOREST | OPT_NOTIE | OPT_NONULL)
//     Only note-attack tokens (when etracting **kern data)
//

void HumdrumFileBase::getTrackSequence(vector<vector<HTp> >& sequence,
		HTp starttoken, int options) {
	int track = starttoken->getTrack();
	getTrackSequence(sequence, track, options);
}


void HumdrumFileBase::getTrackSequence(vector<vector<HTp> >& sequence,
		int track, int options) {
	bool primaryQ   = options & OPT_PRIMARY;
	bool nonullQ    = options & OPT_NONULL;
	bool noemptyQ   = options & OPT_NOEMPTY;
	bool nointerpQ  = options & OPT_NOINTERP;
	bool nomanipQ   = options & OPT_NOMANIP;
	bool nocommentQ = options & OPT_NOCOMMENT;
	bool noglobalQ  = options & OPT_NOGLOBAL;
	bool norestQ    = options & OPT_NOREST;
	bool notieQ     = options & OPT_NOTIE;

	vector<vector<HTp> >& output = sequence;
	output.reserve(getLineCount());
	output.resize(0);

	vector<HTp> tempout;
	auto& infile = *this;
	int i, j;
	bool allNull;
	HTp token;
	bool foundTrack;

	for (i=0; i<infile.getLineCount(); i++) {
		tempout.resize(0);
		if (!noglobalQ && (infile[i].isGlobal())) {
			tempout.push_back(infile[i].token(0));
			output.push_back(tempout);
			continue;
		}
		if (noemptyQ) {
			allNull = true;
			for (j=0; j<infile[i].getFieldCount(); j++) {
				if (infile[i].token(j)->getTrack() != track) {
					continue;
				}
				if (!infile[i].token(j)->isNull()) {
					allNull = false;
					break;
				}
			}
			if (allNull) {
				continue;
			}
		}

		foundTrack = false;
		for (j=0; j<infile[i].getFieldCount(); j++) {
			token = infile[i].token(j);
			if (token->getTrack() != track) {
				continue;
			}
			if (primaryQ && foundTrack) {
				continue;
			}
			foundTrack = true;
			if (nointerpQ && (infile[i].token(j)->isManipulator() ||
					infile[i].token(j)->isTerminator() ||
					infile[i].token(j)->isExclusive())) {
				continue;
			}
			if (nomanipQ && infile[i].token(j)->isManipulator()) {
				continue;
			}
			if (nonullQ && infile[i].token(j)->isNull()) {
				continue;
			}
			if (nocommentQ && infile[i].token(j)->isComment()) {
				continue;
			}
			if (norestQ && infile[i].token(j)->isRest()) {
				continue;
			}
			if (notieQ && infile[i].token(j)->isSecondaryTiedNote()) {
				continue;
			}

			tempout.push_back(infile[i].token(j));
		}
		if (tempout.size() > 0) {
			output.push_back(tempout);
		}
	}
}



//////////////////////////////
//
// HumdrumFileBase::getTrackStart -- Return the starting exclusive
//     interpretation for the given track.  Returns NULL if the track
//     number is out of range.
//

HTp HumdrumFileBase::getTrackStart(int track) const {
	if ((track > 0) && (track < (int)m_trackstarts.size())) {
		return m_trackstarts[track];
	} else {
		return NULL;
	}
}



//////////////////////////////
//
// HumdrumFileBase::getTrackEndCount -- Return the number of ending tokens
//    for the given track.  Spines must start as a single exclusive
//    interpretation token.  However, since spines may split and merge,
//    it is possible that there are more than one termination points for a
//    track.  This function returns the number of terminations which are
//    present in a file for any given spine/track.
//

int HumdrumFileBase::getTrackEndCount(int track) const {
	if (track < 0) {
		track += (int)m_trackends.size();
	}
	if (track < 0) {
		return 0;
	}
	if (track >= (int)m_trackends.size()) {
		return 0;
	}
	return (int)m_trackends[track].size();
}



//////////////////////////////
//
// HumdrumFileBase::getTrackEnd -- Returns a pointer to the terminal manipulator
//    token for the given track and subtrack.  Sub-tracks are indexed from 0 up
//    to but not including getTrackEndCount.
//

HTp HumdrumFileBase::getTrackEnd(int track, int subtrack) const {
	if (track < 0) {
		track += (int)m_trackends.size();
	}
	if (track < 0) {
		return NULL;
	}
	if (track >= (int)m_trackends.size()) {
		return NULL;
	}
	if (subtrack < 0) {
		subtrack += (int)m_trackends[track].size();
	}
	if (subtrack < 0) {
		return NULL;
	}
	if (subtrack >= (int)m_trackends[track].size()) {
		return NULL;
	}
	return m_trackends[track][subtrack];
}



//////////////////////////////
//
// HumdrumFileBase::analyzeLines -- Store a line's index number in the
//    HumdrumFile within the HumdrumLine object at that index.
//    Returns false if there was an error.
//

bool HumdrumFileBase::analyzeLines(void) {
	for (int i=0; i<(int)m_lines.size(); i++) {
		m_lines[i]->setLineIndex(i);
	}
	return isValid();
}



//////////////////////////////
//
// HumdrumFileBase::analyzeTracks -- Analyze the track structure of the
//     data.  Returns false if there was a parse error.
//

bool HumdrumFileBase::analyzeTracks(void) {
	for (int i=0; i<(int)m_lines.size(); i++) {
		int status = m_lines[i]->analyzeTracks(m_parseError);
		if (!status) {
			return false;
		}
	}
	return isValid();
}



//////////////////////////////
//
// HumdrumFileBase::analyzeLinks -- Generate forward and backwards spine links
//    for each token.
//

bool HumdrumFileBase::analyzeLinks(void) {
	HumdrumLine* next     = NULL;
	HumdrumLine* previous = NULL;

	for (int i=0; i<(int)m_lines.size(); i++) {
		if (!m_lines[i]->hasSpines()) {
			continue;
		}
		previous = next;
		next = m_lines[i];
		if (previous != NULL) {
			if (!stitchLinesTogether(*previous, *next)) {
				return isValid();
			}
		}
	}
	return isValid();;
}



//////////////////////////////
//
// HumdrumFileBase::stitchLinesTogether -- Make forward/backward links for
//    tokens on each line.
//

bool HumdrumFileBase::stitchLinesTogether(HumdrumLine& previous,
		HumdrumLine& next) {
	int i;

	// first handle simple cases where the spine assignments are one-to-one:
	if (!previous.isInterpretation() && !next.isInterpretation()) {
		if (previous.getTokenCount() != next.getTokenCount()) {
			stringstream err;
			err << "Error lines " << (previous.getLineNumber())
			    << " and " << (next.getLineNumber()) << " not same length\n";
			err << "Line " << (previous.getLineNumber()) << ": " << previous << endl;
			err << "Line " << (next.getLineNumber()) << ": " << next << endl;
			return setParseError(err);
		}
		for (i=0; i<previous.getTokenCount(); i++) {
			if (next.token(i)) {
				previous.token(i)->makeForwardLink(*next.token(i));
			} else {
				cerr << "Strange error 1" << endl;
			}
		}
		return true;
	}
	int ii = 0;
	for (i=0; i<previous.getTokenCount(); i++) {
		if (!previous.token(i)->isManipulator()) {
			if (next.token(ii) != NULL) {
				previous.token(i)->makeForwardLink(*next.token(ii++));
			} else {
				cerr << "Strange error 2" << endl;
			}
		} else if (previous.token(i)->isSplitInterpretation()) {
			// connect the previous token to the next two tokens.
			if (next.token(ii) != NULL) {
				previous.token(i)->makeForwardLink(*next.token(ii++));
			} else {
				cerr << "Strange error 3" << endl;
			}
			if (next.token(ii) != NULL) {
				previous.token(i)->makeForwardLink(*next.token(ii++));
			} else {
				cerr << "Strange error 4" << endl;
			}
		} else if (previous.token(i)->isMergeInterpretation()) {
			// connect multiple previous tokens which are adjacent *v
			// spine manipulators to the current next token.
			while ((i<previous.getTokenCount()) &&
					previous.token(i)->isMergeInterpretation()) {
				if (next.token(ii) != NULL) {
					previous.token(i)->makeForwardLink(*next.token(ii));
				} else {
					cerr << "Strange error 5" << endl;
				}
				i++;
			}
			i--;
			ii++;
		} else if (previous.token(i)->isExchangeInterpretation()) {
			// swapping the order of two spines.
			if ((i<previous.getTokenCount()) &&
					previous.token(i+1)->isExchangeInterpretation()) {
				if (next.token(ii) != NULL) {
					previous.token(i+1)->makeForwardLink(*next.token(ii++));
				} else {
					cerr << "Strange error 6" << endl;
				}
				if (next.token(ii) != NULL) {
					previous.token(i)->makeForwardLink(*next.token(ii++));
				} else {
					cerr << "Strange error 7" << endl;
				}
			}
			i++;
		} else if (previous.token(i)->isTerminateInterpretation()) {
			// No link should be made.  There may be a problem if a
			// new segment is given (this should be handled by a
			// HumdrumSet class, not HumdrumFileBase.
		} else if (previous.token(i)->isAddInterpretation()) {
			// A new data stream is being added, the next linked token
			// should be an exclusive interpretation.
			if (!next.token(ii+1)->isExclusiveInterpretation()) {
				stringstream err;
				err << "Error: expecting exclusive interpretation on line "
				    << next.getLineNumber() << " at token " << i << " but got "
				    << next.token(i);
				return setParseError(err);
			}
			if (next.token(ii) != NULL) {
				previous.token(i)->makeForwardLink(*next.token(ii++));
			} else {
				cerr << "Strange error 8" << endl;
			}
			ii++;
		} else if (previous.token(i)->isExclusiveInterpretation()) {
			if (next.token(ii) != NULL) {
				if (previous.token(i) != NULL) {
					previous.token(i)->makeForwardLink(*next.token(ii++));
				} else {
					cerr << "Strange error 10" << endl;
				}
			} else {
				cerr << "Strange error 9" << endl;
			}
		} else {
			return setParseError("Error: should not get here");
		}
	}

	if ((i != previous.getTokenCount()) || (ii != next.getTokenCount())) {
		stringstream err;
		err << "Error: cannot stitch lines together due to alignment problem\n";
		err << "Line " << previous.getLineNumber() << ": "
		    << previous << endl;
		err << "Line " << next.getLineNumber() << ": "
		    << next << endl;
		err << "I = " <<i<< " token count " << previous.getTokenCount() << endl;
		err << "II = " <<ii<< " token count " << next.getTokenCount();
		return setParseError(err);
	}

	return isValid();
}



//////////////////////////////
//
// HumdrumFileBase::analyzeSpines -- Analyze the spine structure of the
//     data.  Returns false if there was a parse error.
//

bool HumdrumFileBase::analyzeSpines(void) {
	vector<string> datatype;
	vector<string> sinfo;
	vector<vector<HTp> > lastspine;
	m_trackstarts.resize(0);
	m_trackends.resize(0);
	addToTrackStarts(NULL);

	bool init = false;
	int i, j;
	for (i=0; i<getLineCount(); i++) {
		if (!m_lines[i]->hasSpines()) {
			m_lines[i]->token(0)->setFieldIndex(0);
			continue;
		}
		if ((init == false) && !m_lines[i]->isExclusive()) {
			stringstream err;
			err << "Error on line: " << (i+1) << ':' << endl;
			err << "   Data found before exclusive interpretation" << endl;
			err << "   LINE: " << *m_lines[i];
			return setParseError(err);
		}
		if ((init == false) && m_lines[i]->isExclusive()) {
			// first line of data in file.
			init = true;
			datatype.resize(m_lines[i]->getTokenCount());
			sinfo.resize(m_lines[i]->getTokenCount());
			lastspine.resize(m_lines[i]->getTokenCount());
			for (j=0; j<m_lines[i]->getTokenCount(); j++) {
				datatype[j] = m_lines[i]->getTokenString(j);
				addToTrackStarts(m_lines[i]->token(j));
				sinfo[j]    = to_string(j+1);
				m_lines[i]->token(j)->setSpineInfo(sinfo[j]);
				m_lines[i]->token(j)->setFieldIndex(j);
				lastspine[j].push_back(m_lines[i]->token(j));
			}
			continue;
		}
		if ((int)datatype.size() != m_lines[i]->getTokenCount()) {
			stringstream err;
			err << "Error on line " << (i+1) << ':' << endl;
			err << "   Expected " << datatype.size() << " fields,"
			    << "    but found " << m_lines[i]->getTokenCount();
			err << "\nLine is: " << m_lines[i] << endl;
			if (i > 0) {
				cerr << "Previous line is: " << m_lines[i-1] << endl;
			}
			return setParseError(err);
		}
		for (j=0; j<m_lines[i]->getTokenCount(); j++) {
			m_lines[i]->token(j)->setSpineInfo(sinfo[j]);
			m_lines[i]->token(j)->setFieldIndex(j);
		}
		if (!m_lines[i]->isManipulator()) {
			continue;
		}
		if (!adjustSpines(*m_lines[i], datatype, sinfo)) { return isValid(); }
	}
	return isValid();
}



//////////////////////////////
//
// HumdrumFileBase::addToTrackStarts -- A starting exclusive interpretation was
//    found, so store in the list of track starts.  The first index position
//    in trackstarts is reserve for non-spine usage.
//

void HumdrumFileBase::addToTrackStarts(HTp token) {
	if (token == NULL) {
		m_trackstarts.push_back(NULL);
		m_trackends.resize(m_trackends.size()+1);
	} else if ((m_trackstarts.size() > 1) && (m_trackstarts.back() == NULL)) {
		m_trackstarts.back() = token;
	} else {
		m_trackstarts.push_back(token);
		m_trackends.resize(m_trackends.size()+1);
	}
}



//////////////////////////////
//
// HumdrumFileBase::adjustSpines -- adjust datatype and spineinfo values based
//   on manipulators found in the data.
//

bool HumdrumFileBase::adjustSpines(HumdrumLine& line, vector<string>& datatype,
		vector<string>& sinfo) {
	vector<string> newtype;
	vector<string> newinfo;
	int mergecount = 0;
	int i, j;
	for (i=0; i<line.getTokenCount(); i++) {
		if (line.token(i)->isSplitInterpretation()) {
			newtype.resize(newtype.size() + 1);
			newtype.back() = datatype[i];
			newtype.resize(newtype.size() + 1);
			newtype.back() = datatype[i];
			newinfo.resize(newinfo.size() + 2);
			newinfo[newinfo.size()-2] = '(' + sinfo[i] + ")a";
			newinfo[newinfo.size()-1] = '(' + sinfo[i] + ")b";
		} else if (line.token(i)->isMergeInterpretation()) {
			mergecount = 0;
			for (j=i+1; j<line.getTokenCount(); j++) {
				if (line.token(j)->isMergeInterpretation()) {
					mergecount++;
				} else {
					break;
				}
			}
			newinfo.resize(newtype.size() + 1);
			newinfo.back() = getMergedSpineInfo(sinfo, i, mergecount);
			newtype.resize(newtype.size() + 1);
			newtype.back() = datatype[i];
			i += mergecount;
		} else if (line.token(i)->isAddInterpretation()) {
			newtype.resize(newtype.size() + 1);
			newtype.back() = datatype[i];
			newtype.resize(newtype.size() + 1);
			newtype.back() = "";
			newinfo.resize(newinfo.size() + 1);
			newinfo.back() = sinfo[i];
			newinfo.resize(newinfo.size() + 1);
			addToTrackStarts(NULL);
			newinfo.back() = to_string(getMaxTrack());
		} else if (line.token(i)->isExchangeInterpretation()) {
			if (i < line.getTokenCount() - 1) {
				if (line.token(i)->isExchangeInterpretation()) {
					// exchange spine information
					newtype.resize(newtype.size() + 1);
					newtype.back() = datatype[i+1];
					newtype.resize(newtype.size() + 1);
					newtype.back() = datatype[i];
					newinfo.resize(newinfo.size() + 1);
					newinfo.back() = sinfo[i+1];
					newinfo.resize(newinfo.size() + 1);
					newinfo.back() = sinfo[i];
				} else {
					return setParseError("ERROR1 in *x calculation");
				}
				i++;
			} else {
				stringstream err;
				err << "ERROR2 in *x calculation" << endl;
				err << "Index " << i << " larger than allowed: "
				     << line.getTokenCount() - 1;
				return setParseError(err);
			}
		} else if (line.token(i)->isTerminateInterpretation()) {
			// store pointer to terminate token in trackends
			m_trackends[m_trackstarts.size()-1].push_back(line.token(i));
		} else if (((string*)line.token(i))->substr(0, 2) == "**") {
			newtype.resize(newtype.size() + 1);
			newtype.back() = line.getTokenString(i);
			newinfo.resize(newinfo.size() + 1);
			newinfo.back() = sinfo[i];
			if (!((m_trackstarts.size() > 1) && (m_trackstarts.back() == NULL))) {
				stringstream err;
				err << "Error: Exclusive interpretation with no preparation "
				     << "on line " << line.getLineIndex()
				     << " spine index " << i << endl;
				err << "Line: " << line;
				return setParseError(err);
			}
			if (m_trackstarts.back() == NULL) {
				addToTrackStarts(line.token(i));
			}
		} else {
			// should only be null interpretation, but doesn't matter
			newtype.resize(newtype.size() + 1);
			newtype.back() = datatype[i];
			newinfo.resize(newinfo.size() + 1);
			newinfo.back() = sinfo[i];
		}
	}

	datatype.resize(newtype.size());
	sinfo.resize(newinfo.size());
	for (i=0; i<(int)newtype.size(); i++) {
		datatype[i] = newtype[i];
		sinfo[i]    = newinfo[i];
	}

	return true;
}



//////////////////////////////
//
// HumdrumFileBase::getMergedSpineInfo -- Will only simplify a two-spine
//   merge.  Should be expanded to larger spine mergers in the future.
//   In other words, it is best to currently merge spines in the order
//   in which they were split, so that the original spine label can
//   be produced.
//

string HumdrumFileBase::getMergedSpineInfo(vector<string>& info, int starti,
		int extra) {
	string output;
	int len1;
	int len2;
	if (extra == 1) {
		len1 = (int)info[starti].size();
		len2 = (int)info[starti+1].size();
		if (len1 == len2) {
			if (info[starti].substr(0, len1-1) ==
					info[starti+1].substr(0,len2-1)) {
				output = info[starti].substr(1, len1-3);
				return output;
			}
		}
		output = info[starti] + " " + info[starti+1];
		return output;
	}
	output = info[starti];
	for (int i=0; i<extra; i++) {
		output += " " + info[starti+1+extra];
	}
	return output;
}



//////////////////////////////
//
// HumdrumFileBase::analyzeNonNullDataTokens -- For null data tokens, indicate
//    the previous non-null token which the null token refers to.  After
//    a spine merger, there may be multiple previous tokens, so you would
//		have to decide on the actual source token on based on subtrack or
//    sub-spine information.  The function also gives links to the previous/next
//    non-null tokens, skipping over intervening null data tokens.
//

bool HumdrumFileBase::analyzeNonNullDataTokens(void) {
	vector<HTp> ptokens;

	// analyze forward tokens:
	for (int i=1; i<=getMaxTrack(); i++) {
		if (!processNonNullDataTokensForTrackForward(getTrackStart(i),
				ptokens)) {
			return false;
		}
	}

	ptokens.resize(0);

	// analyze backward tokens:
	for (int i=1; i<=getMaxTrack(); i++) {
		for (int j=0; j<getTrackEndCount(i); j++) {
			if (!processNonNullDataTokensForTrackBackward(getTrackEnd(i, j),
					ptokens)) {
				return false;
			}
		}
	}

	// Eventually set the foward and backward non-null data token for
	// tokens in spines for all types of line types  For now specify
	// the next non-null data token for the exclusive interpretation token.
	// Also this implementation does not consider that the first
	// non-null data tokens may be from nultiple split tokens (fix later).
	vector<HTp> starts;
	vector<HTp> nexts;
	getSpineStartList(starts);
	nexts.resize(starts.size(), NULL);
	for (int i=0; i<(int)starts.size(); i++) {
		if (starts[i] == NULL) {
			continue;
		}
		HTp token = starts[i];
		token = token->getNextToken();
		while (token) {
			if (token->isData()) {
				if (!token->isNull()) {
					nexts[i] = token;
					break;
				}
			}
			token = token->getNextToken();
		}
	}
	for (int i=0; i<(int)nexts.size(); i++) {
		if (nexts[i] == NULL) {
			continue;
		}
		starts[i]->addNextNonNullToken(nexts[i]);
	}

	return true;
}



//////////////////////////////
//
// HumdurmFile::processNonNullDataTokensForTrackBackward -- Helper function
//    for analyzeNonNullDataTokens.  Given any token, this function tells
//    you what is the next non-null data token(s) in the spine after the given
//    token.
//

bool HumdrumFileBase::processNonNullDataTokensForTrackBackward(
		HTp endtoken, vector<HTp> ptokens) {

	HTp token = endtoken;
	int tcount = token->getPreviousTokenCount();

	while (tcount > 0) {
		for (int i=1; i<tcount; i++) {
			if (!processNonNullDataTokensForTrackBackward(
					token->getPreviousToken(i), ptokens)) {
				return false;
			}
		}
		HTp prevtoken = token->getPreviousToken();
		if (prevtoken->isSplitInterpretation()) {
			addUniqueTokens(prevtoken->m_nextNonNullTokens, ptokens);
			if (token != prevtoken->m_nextTokens[0]) {
				// terminate if not most primary subspine
				return true;
			}
		} else if (token->isData()) {
			addUniqueTokens(token->m_nextNonNullTokens, ptokens);
			if (!token->isNull()) {
				ptokens.resize(0);
				ptokens.push_back(token);
			}
		}

		// Follow previous data token 0 since 1 and higher are handled above.
		token = token->getPreviousToken(0);
		tcount = token->getPreviousTokenCount();
	}

	return true;
}



//////////////////////////////
//
// HumdurmFile::processNonNullDataTokensForTrackForward -- Helper function
//    for analyzeNonNullDataTokens.  Given any token, this function tells
//    you what are the previous non-null data token(s) in the spine before
//    the given token.
//

bool HumdrumFileBase::processNonNullDataTokensForTrackForward(HTp starttoken,
		vector<HTp> ptokens) {

	HTp token = starttoken;
	int tcount = token->getNextTokenCount();
	while (tcount > 0) {
		if (token->isSplitInterpretation()) {
			for (int i=1; i<tcount; i++) {
				if (!processNonNullDataTokensForTrackForward(
						token->getNextToken(i), ptokens)) {
					return false;
				}
			}
		} else if (token->isMergeInterpretation()) {
			HTp nexttoken = token->getNextToken();
			addUniqueTokens(nexttoken->m_previousNonNullTokens, ptokens);
			if (token != nexttoken->m_previousTokens[0]) {
				// terminate if not most primary subspine
				return true;
			}
		} else {
			addUniqueTokens(token->m_previousNonNullTokens, ptokens);
			if (token->isData() && !token->isNull()) {
				ptokens.resize(0);
				ptokens.push_back(token);

			}
		}
		// Data tokens can only be followed by up to one next token,
		// so no need to check for more than one next token.
		token = token->getNextToken(0);
		tcount = token->getNextTokenCount();
	}

	return true;
}



//////////////////////////////
//
// HumdrumFileBase::addUniqueTokens -- Used for non-null token analysis.  The
//    analysis is recursive like rhythmic analysis in the HumdrumFileStructure
//    class, but this algorithm does not terminate secondary traversals when
//    recursing.  Perhaps that should be fixed (utilizing the "rhycheck"
//    variable in HumdrumTokens)
//

void HumdrumFileBase::addUniqueTokens(vector<HTp>& target,
		vector<HTp>& source) {
	int i, j;
	bool found;
	for (i=0; i<(int)source.size(); i++) {
		found = false;
		for (j=0; j<(int)target.size(); j++) {
			if (source[i] == target[i]) {
				found = true;
			}
		}
		if (!found) {
			target.push_back(source[i]);
		}
	}
}



//////////////////////////////
//
// HumdrumFileBase::adjustMergeSpineLines -- fix *v lines to that adjacent
//     tracks do not merge at the same time.  In other words, split the line
//     into two or more merge lines.
//

/* still to be implemented

void HumdrumFileBase::adjustMergeSpineLines(void) {
	HumdrumFileBase& infile = *this;
	// going backwards to not have to deal with line number updates
	// at the moment...
	for (int i=infile.getLineCount()-1; i>= 0; i--) {
		if (!infile[i].isManipulator()) {
			continue;
		}
		bool hasbadmerge = false;
		int track1;
		int track2;
		for (int j=1; j<infile[i].getFieldCount(); j++) {
			if (!infile[i].token(j)->equalTo("*v")) {
				continue;
			}
			if (!infile[i].token(j-1)->equalTo("*v")) {
				continue;
			}
			track1 = infile.token(i, j-1)->getTrack();
			track2 = infile.token(i, j)->getTrack();
			if (track1 != track2) {
				hasbadmerge = true;
				break;
			}
		}
		if (hasbadmerge) {
			cerr << "!! BADMERGE on line " << i + 1 << endl;
			fixMerges(i);
		}
	}
}

*/



//////////////////////////////
//
// HumdrumFileBase::fixMerges -- Split a line with merges into two
//    lines.  The line is presumed to have a bad merge which
//    means that two adjacent tracks have adjacent *v tokens.
//    This algorithm will create a new lines where everything
//    after the bad merge is placed on the newline.   Example:
//
// track:    1    2    2    3    3    4    5    5 
//           *    *v   *v   *v   *v   *    *v   *v
//
// This is invalid because track 2 and track 3 have adjacent *v tokens.
// This function will create a new line and move everything after 
// the bad position to a new line:
//
// track:    1    2    2    3    3    4    5    5 
//           *    *v   *v   *v   *v   *    *v   *v
//           *    *    *v   *v   *    *    *
// track:    1    2    3    3    4    5    5 
//
// This algorithm only fixes one bad boundary.  The calling function
// will presumably fix any bad boundaries on the newly created line.
//

/* Still to be implemented...
void HumdrumFileBase::fixMerges(int linei) {
	HumdrumFileBase& infile = *this;

	vector<vector<HTp> > linetoks;
	HTp tok;

	// linetoks: collect tokens on the current line by track groups.
	int track1 = -1;
	int track2 = -1;
	for (int j=0; j<infile[linei].getFieldCount(); j++) {
		tok = infile[linei].token(j);
		track2 = tok->getTrack();
		if (track2 != track1) {
			linetoks.resize(linetoks.size()+1);
			linetoks.back().push_back(tok);
		}
		track1 = track2;
	}

	// ptoks: collect the tokens on the previous line for stiching tokens
	// together after adding new line.
	vector<vector<HTp> > ptoks;
	track1 = -1;
	track2 = -1;
	for (int j=0; j<infile[linei-1].getFieldCount(); j++) {
		tok = infile[linei-1].token(j);
		track2 = tok->getTrack();
		if (track2 != track1) {
			ptoks.resize(ptoks.size()+1);
			ptoks.back().push_back(tok);
		}
		track1 = track2;
	}

	// ntoks: collect the tokens on the next line for stiching tokens
	// together after adding new line.
	vector<vector<HTp> > ntoks;
	track1 = -1;
	track2 = -1;
	for (int j=0; j<infile[linei+1].getFieldCount(); j++) {
		tok = infile[linei+1].token(j);
		track2 = tok->getTrack();
		if (track2 != track1) {
			ntoks.resize(ntoks.size()+1);
			ntoks.back().push_back(tok);
		}
		track1 = track2;
	}

	int maxt = infile.getMaxTrack();
	vector<vector<HTp> > newtokbytrack(maxt+1);

// track:    1    2    2    3    3    4    5    5 
//           *    *v   *v   *v   *v   *    *v   *v
//
// o = new null tokens.
//
// original     *    *v   *v   o    o    o    o    o 
// new          o    o         *v   *v   *    *v   *v
// track:       1    2         3    3    4    5    5 

	HumdrumLine* newline = new HumdrumLine;
	newline->setOwner(this);
	bool foundboundary = false;
	HTp token;
	int findex;
	// int swaptrack = -1;
	int difference = 0;  // decrease in token count on new line
	for (int i=0; i<linetoks.size()-1; i++) {
		if (foundboundary) {
			// transfer the track tokens to the new line, and put
			// new null tokens in their place on the old line.
			for (int j=0; j<(int)linetoks[i].size(); j++) {
				track1 = linetoks[i][j]->getTrack();
				findex = linetoks[i][j]->getFieldIndex();
            // move the token to the next line:
				newline->m_tokens.push_back(linetoks[i][j]);
				// put it in the list for later processing:
				newtokbytrack[track1].push_back(linetoks[i][j]);
				// replace the moved token with a null token:
				token = new HumdrumToken("*");
				infile[linei].m_tokens[findex] = token;
				// probably need to update the HumAddress of both tokens.
			}
		} else if ((!foundboundary) && linetoks[i].back()->equalTo("*v") &&
				linetoks[i+1][0]->equalTo("*v")) {
			// This is the bad boundary.  Keep track fields in the
			// original line, and create one new null token in
			// the newline.
// original     *    *v   *v   o    o    o    o    o 
// new          o    o         *v   *v   *    *v   *v
// track:       1    2         3    3    4    5    5 
			difference = linetoks[i].size() - 1;

			track1 = linetoks[i][0]->getTrack();
			token = new HumdrumToken("*");
			token->setTrack(track1);
			token->setSubtrack(track1);
			newline->m_tokens.push_back(token);
			// put new token in list for later processing:
			newtokbytrack[track1].push_back(token);
			
			foundboundary = true;
		} else {
			// add null tokens to the new line, and keep the
			// tokens on the original line as they were
			for (int j=0; j<(int)linetoks[i].size(); j++) {
				track1 = linetoks[i][j]->getTrack();
				token = new HumdrumToken("*");
				token->setTrack(track1);
				token->setSubtrack(track1);
				newline->m_tokens.push_back(token);
				// put new token in list for later processing:
				newtokbytrack[track1].push_back(token);
			}
		}
	}

	// for now the links between the tokens on successive lines
	// will not be updated.  For the most part it will not be
	// important.  Probably more important is to update line numbers
	// for HumdrumLines occurring on new lines.  Maybe need to set
	// the line type for the new line.

	// add the new line to the file:
	m_lines.insert(m_lines.begin() + linei + 1, newline);

}

*/


//////////////////////////////
//
// operator<< -- Default method of printing HumdrumFiles.  This printing method
//    assumes that the HumdrumLine string is correct.  If a token is changed
//    in the file, the HumdrumFileBase::createLinesFromTokens() before printing
//    the contents of the line.
//

ostream& operator<<(ostream& out, HumdrumFileBase& infile) {
	for (int i=0; i<infile.getLineCount(); i++) {
		out << infile[i] << '\n';
	}
	return out;
}


//////////////////////////////
//
// sortTokenParisByLineIndex -- Sort two tokens so that the one
//    on the smaller line is first.  If both are on the same line then
//    sort the left-most token first.
//

bool sortTokenPairsByLineIndex(const TokenPair& a, const TokenPair& b) {
	if (a.first->getLineIndex() < b.first->getLineIndex()) {
		return true;
	}
	if (a.first->getLineIndex() == b.first->getLineIndex()) {
		if (a.first->getFieldIndex() < b.first->getFieldIndex()) {
			return true;
		}
	}
	return false;
}



//////////////////////////////
//
// HumdrumFileBase::makeBooleanTrackList --
//

void HumdrumFileBase::makeBooleanTrackList(vector<bool>& spinelist,
		const string& spinestring) {
	Convert::makeBooleanTrackList(spinelist, spinestring, getMaxTrack());
}



//////////////////////////////
//
// HumdrumFileBase::getMeasureNumber -- If the current line is a
//      barline, then read the first integer found in the fields on the line.
//

int HumdrumFileBase::getMeasureNumber(int line) {
   HumdrumFileBase& infile = *this;
   int j;
   if (!infile[line].isBarline()) {
      // Return -1 if not a barline.  May be changed in the future
      // to return the measure number of the previous barline.
      return -1;
   }
   HumRegex hre;
   int measurenumber = -1;
   for (j=0; j<infile[line].getFieldCount(); j++) {
      if (hre.search(*infile.token(line, j), "^=[^\\d]*(\\d+)")) {
         measurenumber = hre.getMatchInt(1);
         return measurenumber;
      }
   }
	return -1;
}


// END_MERGE

} // end namespace hum



